{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Sentiment Analysis with Structural Features\n",
      "###Rohit Pathak, Bharadwaj Tanikella\n",
      "We train a classifier with features extracted from a RST parse tree to perform sentiment analysis on user reviews. All features and the intuition behind them are discussed along the way.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from sklearn.linear_model import Perceptron\n",
      "from nltk.classify.scikitlearn import SklearnClassifier\n",
      "from nltk import NaiveBayesClassifier\n",
      "from nltk.classify import apply_features\n",
      "from nltk.tokenize import sent_tokenize, word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "from collections import defaultdict, Counter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Helper methods and miscellaneous stuff to clean up text\n",
      "from HTMLParser import HTMLParser\n",
      "\n",
      "class MLStripper(HTMLParser):\n",
      "    def __init__(self):\n",
      "        self.reset()\n",
      "        self.fed = []\n",
      "    def handle_data(self, d):\n",
      "        self.fed.append(d)\n",
      "    def get_data(self):\n",
      "        return ''.join(self.fed)\n",
      "\n",
      "stripper = MLStripper() # ;)\n",
      "def strip_tags(html):\n",
      "    stripper.feed(html)\n",
      "    return stripper.get_data()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Feature Extractor object computes features for a document."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class FeatureExtractor():\n",
      "    \n",
      "    def __init__(self,):\n",
      "        self.features = {}\n",
      "    \n",
      "    def bow_features(self, text):\n",
      "        w = Counter()\n",
      "        for sent in sent_tokenize(text.decode(\"utf8\")):\n",
      "            words = word_tokenize(sent)\n",
      "            w.update(words)\n",
      "        feats = { ('BOW', word) : val for word,val in w.items() } \n",
      "        return feats\n",
      "            \n",
      "    # TODO: Add RST features here\n",
      "    def rst_features(self, text):\n",
      "        return {}\n",
      "    \n",
      "    def extract(self, review):\n",
      "        text = strip_tags(review)\n",
      "        features = self.bow_features(text)\n",
      "        return features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our dataset contains 50k imdb reviews. Currently we are training a Naive Bayes Classifier with 50 random reviews and testing the accuracy on 50 other randomly selected reviews. The classification task is binary - each review is either POSitive or NEGagive.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import random\n",
      "\n",
      "DATA_DIR = 'aclImdb/'\n",
      "train_files = {'pos': [DATA_DIR + 'train/pos/' + f for f in os.listdir(DATA_DIR+'train/pos/')], \n",
      "               'neg': [DATA_DIR + 'train/neg/' + f for f in os.listdir(DATA_DIR+'train/neg/')]}\n",
      "\n",
      "sample = train_files['pos'] + train_files['neg']\n",
      "random.shuffle(sample)\n",
      "fet = FeatureExtractor()\n",
      "\n",
      "def generate_labelled_data( files ):\n",
      "    data = []\n",
      "    for filename in (files):\n",
      "        label = 'pos' if 'pos' in filename else 'neg'\n",
      "        f = open(filename, 'r')\n",
      "        review = f.read()\n",
      "        f.close()\n",
      "        data.append( (fet.extract(review), label) )\n",
      "        print filename, label.upper(), ' | ',\n",
      "    return data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Generating training set\n",
      "train_set = generate_labelled_data(sample[:50])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/pos/3646_8.txt POS  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/neg/9268_3.txt NEG  |  "
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-72-c98eb463e8b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Generating training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_labelled_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-65-d8c0239ce0a1>\u001b[0m in \u001b[0;36mgenerate_labelled_data\u001b[0;34m(files)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' | '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-36-c143f0600957>\u001b[0m in \u001b[0;36mextract\u001b[0;34m(self, review)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrip_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbow_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-36-c143f0600957>\u001b[0m in \u001b[0;36mbow_features\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'BOW'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/nltk/tokenize/__init__.pyc\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     91\u001b[0m     along with :class:`.PunktSentenceTokenizer`).\n\u001b[1;32m     92\u001b[0m     \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m     return [token for sent in sent_tokenize(text)\n\u001b[0m\u001b[1;32m     94\u001b[0m             for token in _treebank_word_tokenize(sent)]\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/nltk/tokenize/__init__.pyc\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \"\"\"\n\u001b[1;32m     81\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/english.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m         \"\"\"\n\u001b[0;32m-> 1270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1316\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \"\"\"\n\u001b[0;32m-> 1318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1346\u001b[0m         \"\"\"\n\u001b[1;32m   1347\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \"\"\"\n\u001b[1;32m    353\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/neg/1156_2.txt NEG  | "
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Generating test set\n",
      "test_set = generate_labelled_data(sample[200:250])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Generating test set\n",
        "aclImdb/train/neg/1418_4.txt"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " NEG  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/neg/2989_3.txt NEG  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/neg/3636_1.txt NEG  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/neg/3144_1.txt NEG  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/neg/11448_2.txt NEG  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/neg/9260_1.txt NEG  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/pos/4644_10.txt POS  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/pos/380_10.txt POS  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/neg/3586_4.txt NEG  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/neg/3683_1.txt NEG  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/pos/4557_8.txt POS  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/pos/5182_8.txt POS  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/neg/7433_4.txt NEG  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/pos/555_8.txt POS  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/pos/5762_8.txt POS  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/neg/7539_1.txt NEG  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/pos/7912_8.txt POS  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/neg/3756_1.txt NEG  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/neg/2026_2.txt NEG  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/pos/10469_10.txt POS  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/pos/6976_9.txt POS  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/neg/12484_4.txt NEG  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/pos/8264_9.txt POS  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/pos/1824_8.txt POS  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/pos/8607_9.txt POS  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/neg/2929_2.txt NEG  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/neg/10742_1.txt NEG  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/pos/1313_9.txt POS  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/neg/7621_4.txt NEG  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/neg/5598_4.txt NEG  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/neg/6655_3.txt NEG  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/pos/6195_10.txt POS  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/neg/4596_2.txt NEG  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/neg/12400_4.txt NEG  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/neg/4762_1.txt NEG  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/pos/1784_10.txt POS  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/pos/2748_8.txt POS  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/pos/10527_10.txt POS  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/neg/6082_4.txt NEG  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/pos/10822_10.txt POS  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/neg/1324_1.txt NEG  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/pos/10148_10.txt POS  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/neg/2342_1.txt NEG  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/pos/3563_8.txt POS  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/pos/6756_10.txt POS  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/neg/6848_4.txt NEG  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/pos/268_8.txt POS  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/neg/8646_1.txt NEG  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/neg/2683_3.txt NEG  |  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aclImdb/train/pos/12402_7.txt POS  | \n"
       ]
      }
     ],
     "prompt_number": 67
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Training a classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier = NaiveBayesClassifier.train(train_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Accuracy\n",
      "[We will test the accuracy of our classifier incrementally. But for now, it's pretty direct.]"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classifier, test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.54\n"
       ]
      }
     ],
     "prompt_number": 70
    }
   ],
   "metadata": {}
  }
 ]
}