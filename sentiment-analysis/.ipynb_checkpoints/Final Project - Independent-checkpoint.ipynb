{
 "metadata": {
  "name": "",
  "signature": "sha256:44b5fd3b1fb803db361e0f595ef1e26d9836f161f00692ad8a337a9777d2b9e1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Sentiment Analysis with Structural Features\n",
      "###Rohit Pathak, Bharadwaj Tanikella\n",
      "We train a classifier with features extracted from a RST parse tree to perform sentiment analysis on user reviews. All features and the intuition behind them are discussed along the way.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from nltk.classify.scikitlearn import SklearnClassifier\n",
      "from nltk import NaiveBayesClassifier\n",
      "from nltk.classify import apply_features\n",
      "from nltk.tokenize import sent_tokenize, word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.linear_model import Ridge\n",
      "from collections import defaultdict, Counter\n",
      "import re\n",
      "import os\n",
      "import random\n",
      "import utils\n",
      "import math\n",
      "\n",
      "import sys\n",
      "sys.path.insert(0,'../RSTParser')\n",
      "import docparser\n",
      "\n",
      "reload(utils)\n",
      "reload(docparser)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "<module 'docparser' from '../RSTParser/docparser.pyc'>"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Feature Extractor object computes features for a document."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our dataset contains 50k imdb reviews. Currently we are training a Naive Bayes Classifier with 50 random reviews and testing the accuracy on 50 other randomly selected reviews. The classification task is binary - each review is either POSitive or NEGagive.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DATA_DIR = 'data_sample/'\n",
      "\n",
      "train_files = [DATA_DIR + 'train/pos/' + f for f in os.listdir(DATA_DIR+'train/pos/') if f.endswith('.edus')]\n",
      "train_files += [DATA_DIR + 'train/neg/' + f for f in os.listdir(DATA_DIR+'train/neg/') if f.endswith('.edus')]\n",
      "test_files = [DATA_DIR + 'test/pos/' + f for f in os.listdir(DATA_DIR+'test/pos/') if f.endswith('.edus')]\n",
      "test_files += [DATA_DIR + 'test/neg/' + f for f in os.listdir(DATA_DIR+'test/neg/') if f.endswith('.edus')]\n",
      "\n",
      "random.shuffle(train_files)\n",
      "random.shuffle(test_files)\n",
      "\n",
      "def generate_labelled_data( files, feat_func ):\n",
      "    data = []\n",
      "    for i,filename in enumerate(files):\n",
      "        label = 'pos' if 'pos' in filename else 'neg'\n",
      "        f = open(filename, 'r')\n",
      "        review = f.read()\n",
      "        f.close()\n",
      "        f = open(filename.replace('edus','pos'), 'r')\n",
      "        pos_tags = f.read()\n",
      "        f.close()\n",
      "        f = open(filename.replace('edus', 'headwords'), 'r')\n",
      "        headwords = f.read()\n",
      "        f.close()\n",
      "        data.append( (feat_func(review, pos_tags, headwords), label) )\n",
      "        if i >0 and i%100==0: print \"At %s instances\" %i,\n",
      "    return data\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Baseline Features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Bag of Words Features\n",
      "We train a SVM with simple BOW features with stopwords removed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bow_feats(document):\n",
      "    text = document.lower()\n",
      "    words = ['[[BOW]] ' + w for w in re.findall(r'\\w+', text) if not w in stopwords.words('english')]\n",
      "    return dict( Counter(words) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set = generate_labelled_data( train_files, bow_feats )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 900 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1000 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1900 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 2000 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 2100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 2200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 2300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 2400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 2500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 2600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 2700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 2800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 2900 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 3000 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 3100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 3200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 3300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 3400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 3500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 3600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 3700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 3800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 3900 instances\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now train a SVM with the training data generated above."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classif = SklearnClassifier(LinearSVC())\n",
      "classif.train(train_set)\n",
      "classifierNB= nltk.classify.NaiveBayesClassifier.train(train_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And test it on the test data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_set = generate_labelled_data( test_files, bow_feats )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 300 instances\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classif, test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.87\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "precisionRecallF(classif)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'recall': 0.8499999957500001, 'f-measure': 0.8673464345588053, 'precision': 0.8854166620551216}\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classifierNB,test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.8375\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "precisionRecallF(classifierNB)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'recall': 0.8549999957250001, 'f-measure': 0.8402943363138279, 'precision': 0.826086952530981}\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Length of the Document"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Intuition: A very basic feature to see if the length of the review can effect the classification of the sentiment of the review."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def doc_length(document):\n",
      "    text = document.lower()\n",
      "    words = [w for w in re.findall(r'\\w+', text) if not w in stopwords.words('english')]\n",
      "    length={}\n",
      "    length['[[LENGTH]]']=len(words)\n",
      "    return dict(dict( length ).items()+bow_feats(document).items())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set = generate_labelled_data( train_files, doc_length )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 900 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1000 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1900 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 2000 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 2100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 2200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 2300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 2400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 2500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 2600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 2700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 2800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 2900 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 3000 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 3100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 3200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 3300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 3400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 3500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 3600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 3700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 3800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 3900 instances\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classif = SklearnClassifier(LinearSVC())\n",
      "classif.train(train_set)\n",
      "classifierNB= nltk.classify.NaiveBayesClassifier.train(train_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_set = generate_labelled_data( test_files, doc_length )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 300 instances\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classif, test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.8725\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "precisionRecallF(classif)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'recall': 0.8449999957750001, 'f-measure': 0.8688940974751918, 'precision': 0.8941798894487837}\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classifierNB,test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.84\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "precisionRecallF(classifierNB)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'recall': 0.8549999957250001, 'f-measure': 0.8423640279796216, 'precision': 0.8300970833490433}\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Moving on to RST features (individual)\n",
      "We now individually try a range of features extracted from the parse tree to see how they compare against the pure BOW model."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Nucleus Bag of Words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def nucleus_bow_feats(document, tree=None, nodelist=None):\n",
      "    if tree is None and nodelist is None:\n",
      "        tree, nodelist = docparser.getParseTree(document)\n",
      "    for node in nodelist:\n",
      "        if node.prop == 'Nucleus':\n",
      "            text = node.text.lower()\n",
      "            words = ['[[NUC_BOW]] ' + w for w in re.findall(r'\\w+', text) if not w in stopwords.words('english')]\n",
      "            return dict(dict( Counter(words)).items()+ bow_feats(document).items() +doc_length(document).items())\n",
      "    else:\n",
      "        print \"Empty Doc?\",\n",
      "        return bow_feats(document)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 227
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set = generate_labelled_data( train_files, nucleus_bow_feats )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 900 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1000 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? At 1100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1900 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc?\n"
       ]
      }
     ],
     "prompt_number": 228
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_set = generate_labelled_data( test_files, nucleus_bow_feats )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc?\n"
       ]
      }
     ],
     "prompt_number": 229
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classif = SklearnClassifier(LinearSVC())\n",
      "classif.train(train_set)\n",
      "classifierNB= nltk.classify.NaiveBayesClassifier.train(train_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 230
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classif, test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.745\n"
       ]
      }
     ],
     "prompt_number": 231
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "precisionRecallF(classif)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'recall': 0.7299999927, 'f-measure': 0.7411162438612975, 'precision': 0.7525773118290999}\n"
       ]
      }
     ],
     "prompt_number": 232
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classifierNB,test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.805\n"
       ]
      }
     ],
     "prompt_number": 233
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "precisionRecallF(classifierNB)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'recall': 0.8399999916, 'f-measure': 0.8115936956291435, 'precision': 0.7850467216350774}\n"
       ]
      }
     ],
     "prompt_number": 234
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Number of Relations\n",
      "Our proposition is that more of certain kinds of relations are usually positive reviews."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def num_relations_feats(document, tree=None, nodelist=None):\n",
      "    \n",
      "    if tree is None and nodelist is None:\n",
      "        tree, nodelist = docparser.getParseTree(document)\n",
      "    relation_count=defaultdict(int)\n",
      "    relation_count['[[NUM_RELATION]] OFFSET'] = 1\n",
      "    for node in nodelist:\n",
      "        if node.relation is not None:\n",
      "            relation_count[ '[[Num_Relation]] ' + node.relation]+=1\n",
      "    return dict(dict(relation_count).items()+bow_feats(document).items()+doc_length(document).items())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 236
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set = generate_labelled_data( train_files, num_relations_feats )\n",
      "\n",
      "classif = SklearnClassifier(LinearSVC())\n",
      "classif.train(train_set)\n",
      "classifierNB= nltk.classify.NaiveBayesClassifier.train(train_set)\n",
      "\n",
      "test_set = generate_labelled_data( test_files, num_relations_feats )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 900 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1000 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1900 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 100 instances\n"
       ]
      }
     ],
     "prompt_number": 237
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classif, test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.745\n"
       ]
      }
     ],
     "prompt_number": 238
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "precisionRecallF(classif)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'recall': 0.7599999924, 'f-measure': 0.7487679656389079, 'precision': 0.7378640705061741}\n"
       ]
      }
     ],
     "prompt_number": 239
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classifierNB,test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.79\n"
       ]
      }
     ],
     "prompt_number": 240
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "precisionRecallF(classifierNB)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'recall': 0.8099999919, 'f-measure': 0.7941171394659049, 'precision': 0.7788461463572486}\n"
       ]
      }
     ],
     "prompt_number": 241
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Relation BOW\n",
      "Intuition: words in combination of the relation that they can be more reflective of the sentiment that they are associated with. If a word is consistently in a relation, it may affect the 'degree' of that sentiment."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def relation_bow_feats(document, tree=None, nodelist=None):\n",
      "    if tree is None and nodelist is None:\n",
      "        tree, nodelist = docparser.getParseTree(document)\n",
      "    relation_bow = Counter()\n",
      "    relation_bow['[[RELATION_WEIGHT]] OFFSET'] = 1 \n",
      "    for node in nodelist:\n",
      "        if node.relation is not None:\n",
      "            text= node.text.lower()\n",
      "            words = ['[[RELATION_WEIGHT]] ' + node.relation + ' ' + w for w in re.findall(r'\\w+', text) if not w in stopwords.words('english')]\n",
      "            relation_bow.update(words)\n",
      "    #print relation_bow\n",
      "    return dict(dict(relation_bow).items()+bow_feats(document).items()+ doc_length(document).items())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 242
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set = generate_labelled_data( train_files, relation_bow_feats )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 900 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1000 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1900 instances\n"
       ]
      }
     ],
     "prompt_number": 243
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classif = SklearnClassifier(LinearSVC())\n",
      "classif.train(train_set)\n",
      "classifierNB= nltk.classify.NaiveBayesClassifier.train(train_set)\n",
      "\n",
      "test_set = generate_labelled_data( test_files, relation_bow_feats )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 100 instances\n"
       ]
      }
     ],
     "prompt_number": 244
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classif, test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.725\n"
       ]
      }
     ],
     "prompt_number": 245
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "precisionRecallF(classif)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'recall': 0.7299999927, 'f-measure': 0.7263676519891564, 'precision': 0.7227722700715618}\n"
       ]
      }
     ],
     "prompt_number": 246
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classifierNB,test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.8\n"
       ]
      }
     ],
     "prompt_number": 247
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "precisionRecallF(classifierNB)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'recall': 0.7699999923, 'f-measure': 0.7938139252846167, 'precision': 0.8191489274558625}\n"
       ]
      }
     ],
     "prompt_number": 248
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Level of the Tree and BOW"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Intuition: An updated bag of words approachto find the similarity between the level in the tree and the connection between the frequency of words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def depth_bow(document, tree=None, nodelist=None):\n",
      "    if tree is None and nodelist is None:\n",
      "        tree, nodelist = docparser.getParseTree(document)\n",
      "    depth_bow = Counter()\n",
      "    depth_bow['[[DEPTH_BOW]] OFFSET'] = 1 \n",
      "    for i,node in enumerate(nodelist):\n",
      "        depth = math.floor(math.log(i+1,2))\n",
      "        depth = int(depth)\n",
      "        if node.text is not None:\n",
      "            text= node.text.lower()\n",
      "            words = ['[[DEPTH_BOW]] ' + str(depth) + ' ' + w for w in re.findall(r'\\w+', text) if not w in stopwords.words('english')]\n",
      "            depth_bow.update(words)\n",
      "    #print relation_bow\n",
      "    return dict(dict(depth_bow).items()+bow_feats(document).items())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 249
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set = generate_labelled_data( train_files, depth_bow )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 900 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1000 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1900 instances\n"
       ]
      }
     ],
     "prompt_number": 250
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classif = SklearnClassifier(LinearSVC())\n",
      "classif.train(train_set)\n",
      "classifierNB= nltk.classify.NaiveBayesClassifier.train(train_set)\n",
      "\n",
      "test_set = generate_labelled_data( test_files, depth_bow )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 100 instances\n"
       ]
      }
     ],
     "prompt_number": 251
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classif, test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.755\n"
       ]
      }
     ],
     "prompt_number": 252
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "precisionRecallF(classif)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'recall': 0.7899999921, 'f-measure': 0.763284517351957, 'precision': 0.7383177501091799}\n"
       ]
      }
     ],
     "prompt_number": 253
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classifierNB,test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.82\n"
       ]
      }
     ],
     "prompt_number": 254
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "precisionRecallF(classifierNB)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'recall': 0.8499999915, 'f-measure': 0.8252422108590075, 'precision': 0.8018867848878606}\n"
       ]
      }
     ],
     "prompt_number": 255
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Weight of the Relations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Intuition: An experimental way to test how the weight of a particular relation in the parse tree effect the classification. Null Hypothesis: A stronger (pos/neg) review has more elaboration. \n",
      "\n",
      "Answer: The hypothesis has been proven wrong. Value of 0.5 accuracy with a F- measure of 0.019."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def relation_weight_feats(document, tree=None, nodelist=None):\n",
      "    if tree is None and nodelist is None:\n",
      "        tree, nodelist = docparser.getParseTree(document)\n",
      "    relation_weight = Counter()\n",
      "    relation_weight['[[RELATION_BOW]] OFFSET'] = 1 \n",
      "    for node in nodelist:\n",
      "        if node.relation is not None:\n",
      "            text = node.text.lower()\n",
      "            relation_weight['[[RELATION_BOW]] ' + node.relation ] += len(text)\n",
      "    #print relation_bow\n",
      "    return dict(dict(relation_weight).items()+bow_feats(document).items())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 141
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set = generate_labelled_data( train_files, relation_weight_feats )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 900 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1000 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1900 instances\n"
       ]
      }
     ],
     "prompt_number": 142
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classif = SklearnClassifier(LinearSVC())\n",
      "classif.train(train_set)\n",
      "classifierNB= nltk.classify.NaiveBayesClassifier.train(train_set)\n",
      "\n",
      "test_set = generate_labelled_data( test_files, relation_weight_feats )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 100 instances\n"
       ]
      }
     ],
     "prompt_number": 143
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classif, test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.645\n"
       ]
      }
     ],
     "prompt_number": 144
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "precisionRecallF(classif)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'recall': 0.7599999924, 'f-measure': 0.6816138489818386, 'precision': 0.6178861738383238}\n"
       ]
      }
     ],
     "prompt_number": 145
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classifierNB,test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.78\n"
       ]
      }
     ],
     "prompt_number": 146
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "precisionRecallF(classifierNB)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'recall': 0.7799999922, 'f-measure': 0.7799994922003206, 'precision': 0.7799999922}\n"
       ]
      }
     ],
     "prompt_number": 147
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Polarity Features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Intuition: The main intuition behind this is to use the polarity of the words to efficiently tag the nucleus whether the nucleus has more number of positive or negative polarity. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "poswords = set()\n",
      "negwords = set()\n",
      "with open('sentiment-vocab.tff','r') as fin:\n",
      "    for i,line in enumerate(fin):\n",
      "        # more list and dict comprehensions!\n",
      "        kvs = {key:val for key,val in [kvp.split('=') for kvp in line.split() if '=' in kvp]}\n",
      "        if kvs['type'] == 'strongsubj':\n",
      "            if kvs['priorpolarity'] == 'negative':\n",
      "                negwords.add(kvs['word1'])\n",
      "            if kvs['priorpolarity'] == 'positive':\n",
      "                poswords.add(kvs['word1'])\n",
      "\n",
      "all_labels = ['pos', 'neg']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def nucleus_polarity(document, tree=None, nodelist=None):\n",
      "    if tree is None and nodelist is None:\n",
      "        tree, nodelist = docparser.getParseTree(document)\n",
      "    nucleus_polarity = Counter()\n",
      "    nucleus_polarity['[[NUCLEUS_POLARITY]] OFFSET'] = 1 \n",
      "    for node in nodelist:\n",
      "        if node.prop == 'Nucleus':\n",
      "            text = node.text.lower()\n",
      "            words = [ w for w in re.findall(r'\\w+', text) if not w in stopwords.words('english')]\n",
      "            for word in words:\n",
      "                if word in poswords:\n",
      "                    nucleus_polarity['[[NUCLEUS_POLARITY]] POS']+=1\n",
      "                if word in negwords:\n",
      "                    nucleus_polarity['[[NUCLEUS_POLARITY]] NEG']+=1\n",
      "                else:\n",
      "                    nucleus_polarity['[[NUCLEUS_POLARITY]] NEU']+=0.5\n",
      "        return dict(dict(nucleus_polarity).items()+bow_feats(document).items())\n",
      "    else:\n",
      "        print \"Empty Doc?\",\n",
      "        return bow_feats(document)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 277
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set = generate_labelled_data( train_files, nucleus_polarity )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 900 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1000 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1900 instances\n"
       ]
      }
     ],
     "prompt_number": 278
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classif = SklearnClassifier(LinearSVC())\n",
      "classif.train(train_set)\n",
      "classifierNB= nltk.classify.NaiveBayesClassifier.train(train_set)\n",
      "\n",
      "test_set = generate_labelled_data( test_files, nucleus_polarity )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 100 instances\n"
       ]
      }
     ],
     "prompt_number": 279
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classif, test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.755\n"
       ]
      }
     ],
     "prompt_number": 280
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "precisionRecallF(classif)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'recall': 0.7599999924, 'f-measure': 0.7562183979607774, 'precision': 0.7524752400745026}\n"
       ]
      }
     ],
     "prompt_number": 281
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classifierNB,test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.785\n"
       ]
      }
     ],
     "prompt_number": 282
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "precisionRecallF(classifierNB)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'recall': 0.799999992, 'f-measure': 0.7881768322457003, 'precision': 0.7766990215854465}\n"
       ]
      }
     ],
     "prompt_number": 283
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Level Centric Polarity Relations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Intuition: The main reason to find the level centric polarity relations is to see if there is a high correlation between the level the edu's are located and the relative polarity(sentiment) of them. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def level_edu_polarity(document, tree=None, nodelist=None):\n",
      "    if tree is None and nodelist is None and not len(document)==0:\n",
      "        tree, nodelist = docparser.getParseTree(document)\n",
      "    level_edu_polarityDict = Counter()\n",
      "    level_edu_polarityDict['[[LEVEl_RELATION_POLARITY]] OFFSET'] = 1 \n",
      "    depth = math.floor(math.log(len(nodelist)+1,2))\n",
      "    for i,node in enumerate(nodelist): \n",
      "        current_depth= math.floor(math.log(i+1,2)) \n",
      "        if node.prop is not None and current_depth<depth-1 and depth > 1:\n",
      "            text = node.text.lower()\n",
      "            words = [ w for w in re.findall(r'\\w+', text) if not w in stopwords.words('english')]\n",
      "            for word in words:\n",
      "                if word in poswords:\n",
      "                    level_edu_polarityDict['[[LEVEl_RELATION_POLARITY]]'+ node.relation +str(current_depth)+' POS']+=1\n",
      "                if word in negwords:\n",
      "                    level_edu_polarityDict['[[LEVEl_RELATION_POLARITY]]'+ node.relation +str(current_depth)+' NEG']+=1\n",
      "\n",
      "    \n",
      "    return dict(dict(level_edu_polarityDict).items() + bow_feats(document).items() + doc_length(document).items())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set = generate_labelled_data( train_files, level_edu_polarity )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "data_sample/train/pos/8514_7.edus\n",
        "data_sample/train/neg/476_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/6806_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/4035_10.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/10990_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/1742_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/11492_7.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/8225_8.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/259_8.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/639_10.edus\n",
        "data_sample/train/neg/4193_3.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/12049_10.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/653_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/4371_8.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/1108_7.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/7145_9.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/1160_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/2014_7.edus\n",
        "data_sample/train/neg/6001_2.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/2346_9.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/6024_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/4224_10.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/2117_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/7221_8.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/856_7.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/10484_2.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/3387_10.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/2503_1.edus\n",
        "data_sample/train/neg/4313_4.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/4529_4.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/6639_10.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/280_8.edus\n",
        "data_sample/train/neg/10527_4.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/1553_10.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/3303_10.edus\n",
        "data_sample/train/pos/2038_7.edus\n",
        "data_sample/train/pos/5178_10.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/167_7.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/12026_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/9211_2.edus\n",
        "data_sample/train/pos/7084_10.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/7814_4.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/2098_3.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/6173_1.edus\n",
        "data_sample/train/pos/9942_7.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/4666_8.edus\n",
        "data_sample/train/pos/2826_10.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/9541_8.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/1073_9.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/5899_3.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/9499_4.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/6795_9.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/5081_4.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/8133_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/10138_3.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/2790_4.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/7140_10.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/10893_3.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/10763_3.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/8861_9.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/6790_3.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/5305_7.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/5177_8.edus\n",
        "data_sample/train/neg/7943_4.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/10757_10.edus\n",
        "data_sample/train/neg/2683_3.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/4970_3.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/1936_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/11686_10.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/4581_1.edus\n",
        "data_sample/train/neg/5662_3.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/6755_7.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/11413_10.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/2835_7.edus\n",
        "data_sample/train/pos/1643_10.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/9003_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/10132_9.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/2361_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/5133_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/1843_9.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/10177_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/5799_1.edus\n",
        "data_sample/train/neg/5769_3.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/5701_8.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/6624_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/7674_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/3303_4.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/11224_3.edus\n",
        "data_sample/train/neg/4545_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/155_3.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/80_9.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/5340_3.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/7105_9.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/3585_9.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/5619_3.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/1011_2.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/12422_8.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/1308_1.edus\n",
        "data_sample/train/neg/12339_4.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/3835_9.edus\n",
        "data_sample/train/pos/3589_8.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "At 100 instances data_sample/train/neg/11383_2.edus\n",
        "data_sample/train/neg/7857_2.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/1958_4.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/1072_4.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/9045_9.edus\n",
        "data_sample/train/neg/6684_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/4578_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/4647_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/263_4.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/10616_7.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/10152_9.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/9684_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/10485_8.edus\n",
        "data_sample/train/neg/5236_2.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/11106_10.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/10374_8.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/338_4.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/8404_10.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/6666_7.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/8435_10.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/8782_4.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/330_10.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/1424_3.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/9299_3.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/7041_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/122_9.edus\n",
        "data_sample/train/pos/6651_10.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/7352_10.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/75_8.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/7716_9.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/7847_9.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/7181_10.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/5732_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/12423_2.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/7668_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/6354_10.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/4907_3.edus\n",
        "data_sample/train/pos/11004_8.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/9717_3.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/11196_7.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/3585_2.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/6433_3.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/5640_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/7279_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/11579_2.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/4067_8.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/11761_10.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/7644_2.edus\n",
        "data_sample/train/pos/3968_10.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/12119_3.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/10451_10.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/1932_10.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/6564_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/8114_3.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/980_4.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/11008_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/6863_7.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/11334_10.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/11548_10.edus\n",
        "data_sample/train/neg/8229_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/3643_10.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/2658_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/3324_7.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/7841_4.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/10768_1.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/7196_4.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/neg/6046_2.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/2736_9.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/3923_10.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/10663_8.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/1339_9.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data_sample/train/pos/196_9.edus"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "IndexError",
       "evalue": "list index out of range",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-33-665bbef4b8e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_labelled_data\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtrain_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel_edu_polarity\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-31-6e1e4f5c639c>\u001b[0m in \u001b[0;36mgenerate_labelled_data\u001b[0;34m(files, feat_func)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#         bow_features = dict(bow_feats(review).items() + nucleus_bow_feats(review).items() + relation_bow_feats(review).items())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeat_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"At %s instances\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-32-8d4361a5bed9>\u001b[0m in \u001b[0;36mlevel_edu_polarity\u001b[0;34m(document, tree, nodelist)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlevel_edu_polarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodelist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnodelist\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetParseTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlevel_edu_polarityDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlevel_edu_polarityDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'[[LEVEl_RELATION_POLARITY]] OFFSET'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/Bharadwaj/Documents/GeorgiaTech/CS4650/FinalProject/RSTParser/docparser.pyc\u001b[0m in \u001b[0;36mgetParseTree\u001b[0;34m(document)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetParseTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mpred_rst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mnodelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuildtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBFTbin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_rst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgettree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpred_rst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodelist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/Bharadwaj/Documents/GeorgiaTech/CS4650/FinalProject/RSTParser/docparser.pyc\u001b[0m in \u001b[0;36mparse\u001b[0;34m(pm, textedus)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0medus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mpred_rst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msr_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpred_rst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/Bharadwaj/Documents/GeorgiaTech/CS4650/FinalProject/RSTParser/model.pyc\u001b[0m in \u001b[0;36msr_parse\u001b[0;34m(self, texts, pos)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# same arguments as in data generation part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeatureGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;31m# Enumerate through all possible actions ranked based on predcition scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/Bharadwaj/Documents/GeorgiaTech/CS4650/FinalProject/RSTParser/feature.pyc\u001b[0m in \u001b[0;36mfeatures\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;31m# Lexical features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfeat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexical_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/Bharadwaj/Documents/GeorgiaTech/CS4650/FinalProject/RSTParser/feature.pyc\u001b[0m in \u001b[0;36mlexical_features\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'End-Word-StackSpan2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstackspan2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueuespan1\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Begin-Word-QueueSpan1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueuespan1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'End-Word-QueueSpan1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueuespan1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstackspan1\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstackspan2\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIndexError\u001b[0m: list index out of range"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classif = SklearnClassifier(LinearSVC())\n",
      "classif.train(train_set)\n",
      "classifierNB= nltk.classify.NaiveBayesClassifier.train(train_set)\n",
      "\n",
      "test_set = generate_labelled_data( test_files, level_edu_polarity )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 100 instances\n"
       ]
      }
     ],
     "prompt_number": 267
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classif, test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.78\n"
       ]
      }
     ],
     "prompt_number": 272
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "precisionRecallF(classif)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'recall': 0.7699999923, 'f-measure': 0.7777772699727733, 'precision': 0.7857142776967931}\n"
       ]
      }
     ],
     "prompt_number": 273
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classifierNB,test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.785\n"
       ]
      }
     ],
     "prompt_number": 274
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "precisionRecallF(classifierNB)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'recall': 0.7899999921, 'f-measure': 0.7860691439323991, 'precision': 0.7821782100774435}\n"
       ]
      }
     ],
     "prompt_number": 275
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Sentiment Word Net Features. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Usage of SWN http://sentiwordnet.isti.cnr.it/ resource to label the words with weights of Positive and Negative polarity. For our implementation we considered the positive and negative scores of the word. We tried to implement the sentimental weights to improve the features regarding the relation. \n",
      "\n",
      "Intuition: The main reason we tried to implement the relational sentimental weights is because, we thought that the main polarity between different relations can help us find a pattern between the data sets. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#def sentimentWeights(word)\n",
      "\n",
      "with open('SentiWordNet_3.0.0_20130122.txt','r') as fin:\n",
      "    word_sentiment_weight=defaultdict(float)\n",
      "    num_words  = defaultdict(int)\n",
      "    j=0\n",
      "    for i,line in enumerate(fin):\n",
      "        if len(line)>50 and not '# ' in line:\n",
      "            tokens = word_tokenize(line)\n",
      "            for value in tokens:\n",
      "                if value =='#':\n",
      "                    word_sentiment_weight[(tokens[tokens.index(value)-1],'POS')]+=float(tokens[2])\n",
      "                    num_words[tokens[tokens.index(value)-1]]+=1\n",
      "                    word_sentiment_weight[(tokens[tokens.index(value)-1],'NEG')]+=float(tokens[3])\n",
      "    for value in num_words.keys():\n",
      "        word_sentiment_weight[value,'POS']=word_sentiment_weight[value,'POS']/num_words[value]\n",
      "        word_sentiment_weight[value,'NEG']=word_sentiment_weight[value,'NEG']/num_words[value]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def relation_sentiment_weight_feats(document, tree=None, nodelist=None):\n",
      "    if tree is None and nodelist is None:\n",
      "        tree, nodelist = docparser.getParseTree(document)\n",
      "    relation_sentiment_weight = Counter()\n",
      "    relation_sentiment_weight['[[RELATION_SENT_WEIGHT]] OFFSET'] = 1 \n",
      "    for i,node in enumerate(nodelist):\n",
      "        level = math.floor(math.log(i+1,2))\n",
      "        level = int(level)\n",
      "        \n",
      "        if node.relation is not None:\n",
      "            \n",
      "            text = node.text.lower()\n",
      "            words = [w for w in re.findall(r'\\w+', text) if not w in stopwords.words('english')]\n",
      "            \n",
      "            for word in words:\n",
      "                relation_sentiment_weight['[[RELATION_SENT_WEIGHT]] ' + node.relation  + 'POS'] += word_sentiment_weight[word,'POS']\n",
      "                relation_sentiment_weight['[[RELATION_SENT_WEIGHT]] ' + node.relation  + 'NEG'] += word_sentiment_weight[word,'NEG']\n",
      "    return dict(dict(relation_sentiment_weight).items()+ bow_feats(document).items())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set = generate_labelled_data( train_files, relation_sentiment_weight_feats )\n",
      "\n",
      "classif = SklearnClassifier(LinearSVC())\n",
      "classif.train(train_set)\n",
      "classifierNB= nltk.classify.NaiveBayesClassifier.train(train_set)\n",
      "\n",
      "test_set = generate_labelled_data( test_files, relation_sentiment_weight_feats )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 900 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1000 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1900 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 100 instances\n"
       ]
      }
     ],
     "prompt_number": 97
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classif, test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.75\n"
       ]
      }
     ],
     "prompt_number": 98
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "precisionRecallF(classif)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'recall': 0.7499999925, 'f-measure': 0.7499994925003335, 'precision': 0.7499999925}\n"
       ]
      }
     ],
     "prompt_number": 103
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classifierNB,test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.78\n"
       ]
      }
     ],
     "prompt_number": 99
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "precisionRecallF(classifierNB)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'recall': 0.7899999921, 'f-measure': 0.7821777101267778, 'precision': 0.7745097963283354}\n"
       ]
      }
     ],
     "prompt_number": 104
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Parts of Speech Tagging Features for Relations in RST Parse Tree."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Intuition: The main reason for these particular features is because of the relationship between the parse tree relations and a skewed model of POS tags."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def relation_POS(document, tree=None, nodelist=None):\n",
      "    if tree is None and nodelist is None:\n",
      "        tree, nodelist = docparser.getParseTree(document)\n",
      "    relation_POS = Counter()\n",
      "    relation_POS['[[RELATION_POS]] OFFSET'] = 1 \n",
      "    for node in nodelist:\n",
      "        if node.relation is not None:\n",
      "            text = node.text.lower()\n",
      "            for sent in sent_tokenize(text.decode(\"utf8\")):\n",
      "                for (word,tag) in nltk.pos_tag(sent):\n",
      "                        relation_POS['[[RELATION_POS]]'+node.relation+' '+tag]+=1\n",
      "        \n",
      "            \n",
      "    return dict(dict(relation_POS).items()+ bow_feats(document).items())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 156
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set = generate_labelled_data( train_files, relation_POS )\n",
      "\n",
      "classif = SklearnClassifier(LinearSVC())\n",
      "classif.train(train_set)\n",
      "classifierNB= nltk.classify.NaiveBayesClassifier.train(train_set)\n",
      "\n",
      "test_set = generate_labelled_data( test_files, relation_POS )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 500 instances"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-157-63cd1ef03022>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_labelled_data\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtrain_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelation_POS\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclassif\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSklearnClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclassif\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclassifierNB\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNaiveBayesClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-2-eaf1adde222f>\u001b[0m in \u001b[0;36mgenerate_labelled_data\u001b[0;34m(files, feat_func)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#         bow_features = dict(bow_feats(review).items() + nucleus_bow_feats(review).items() + relation_bow_feats(review).items())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeat_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"At %s instances\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-156-793fc1eb28f2>\u001b[0m in \u001b[0;36mrelation_POS\u001b[0;34m(document, tree, nodelist)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                         \u001b[0mrelation_POS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'[[RELATION_POS]]'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelation\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/Bharadwaj/anaconda/lib/python2.7/site-packages/nltk/tag/__init__.pyc\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \"\"\"\n\u001b[1;32m    100\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_POS_TAGGER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpos_tag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/Bharadwaj/anaconda/lib/python2.7/site-packages/nltk/tag/sequential.pyc\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/Bharadwaj/anaconda/lib/python2.7/site-packages/nltk/tag/sequential.pyc\u001b[0m in \u001b[0;36mtag_one\u001b[0;34m(self, tokens, index, history)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtagger\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_taggers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/Bharadwaj/anaconda/lib/python2.7/site-packages/nltk/tag/sequential.pyc\u001b[0m in \u001b[0;36mchoose_tag\u001b[0;34m(self, tokens, index, history)\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0;31m# higher than that cutoff first; otherwise, return None.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cutoff_prob\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mpdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob_classify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/Bharadwaj/anaconda/lib/python2.7/site-packages/nltk/classify/maxent.pyc\u001b[0m in \u001b[0;36mclassify\u001b[0;34m(self, featureset)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob_classify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprob_classify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/Bharadwaj/anaconda/lib/python2.7/site-packages/nltk/classify/maxent.pyc\u001b[0m in \u001b[0;36mprob_classify\u001b[0;34m(self, featureset)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;31m# Normalize the dictionary to give a probability distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         return DictionaryProbDist(prob_dict, log=self._logarithmic,\n\u001b[0;32m--> 165\u001b[0;31m                                   normalize=True)\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/Bharadwaj/anaconda/lib/python2.7/site-packages/nltk/probability.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, prob_dict, log, normalize)\u001b[0m\n\u001b[1;32m    544\u001b[0m                              'before it can be normalized.')\n\u001b[1;32m    545\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m                 \u001b[0mvalue_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prob_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mvalue_sum\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0m_NINF\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m                     \u001b[0mlogp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/Bharadwaj/anaconda/lib/python2.7/site-packages/nltk/probability.pyc\u001b[0m in \u001b[0;36msum_logs\u001b[0;34m(logs)\u001b[0m\n\u001b[1;32m   1974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1975\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msum_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1976\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_NINF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1978\u001b[0m \u001b[0;31m##//////////////////////////////////////////////////////\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/Bharadwaj/anaconda/lib/python2.7/site-packages/nltk/probability.pyc\u001b[0m in \u001b[0;36madd_logs\u001b[0;34m(logx, logy)\u001b[0m\n\u001b[1;32m   1971\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1972\u001b[0m     \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1973\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbase\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogy\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1975\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msum_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 157
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classif, test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "precisionRecallF(classif)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classifierNB,test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "precisionRecallF(classifierNB)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Accuracy\n",
      "[We will test the accuracy of our classifier incrementally. But for now, it's pretty direct.]"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Generating test set\n",
      "test_set = []\n",
      "for inst,l in generate_labelled_data( sorted((test_files['pos'] + test_files['neg']), key=lambda k: random.random()) ):\n",
      "    test_set.append( (inst,l) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "list indices must be integers, not str",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-44-41eed699b517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Generating test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerate_labelled_data\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pos'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtest_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'neg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtest_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mTypeError\u001b[0m: list indices must be integers, not str"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Precision, Recall and F-measure"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def precisionRecallF(classifier):\n",
      "    tp= 0.0\n",
      "    tn= 0.0\n",
      "    fp= 0.0\n",
      "    fn= 0.0\n",
      "\n",
      "    results = classifier.classify_many([fs for (fs,l) in test_set])\n",
      "    actual = [l for (f,l) in test_set]\n",
      "\n",
      "    i=0\n",
      "\n",
      "    for i in range(len(results)):\n",
      "        if results[i]=='pos' and actual[i]=='pos':\n",
      "            tp+=1.0\n",
      "        if results[i]=='pos' and actual[i]=='neg':\n",
      "            fp+=1.0\n",
      "        if results[i]=='neg' and actual[i]=='pos':\n",
      "            fn+=1.0\n",
      "        if results[i]=='neg' and actual[i]=='neg':\n",
      "            tn+=1.0\n",
      "\n",
      "    recall = tp / (tp + fn+1e-6)\n",
      "    precision = tp / (tp + fp + 1e-6)\n",
      "    fmeasure = 2 * recall * precision / (recall + precision + 1e-6)\n",
      "    quality = {'f-measure': fmeasure, 'recall': recall, 'precision' : precision}\n",
      "\n",
      "    print quality"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##LEFTOVERS"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class FeatureExtractor():\n",
      "    \n",
      "    def __init__(self,):\n",
      "        pass\n",
      "    \n",
      "    def bow_features(self, document):\n",
      "        text = document.lower()\n",
      "        words = ['[[BOW]] '+ w for w in re.findall(r'\\w+', text) if not w in stopwords.words('english')]\n",
      "        return dict( Counter(words) )\n",
      "    \n",
      "    # TODO: Add RST features here\n",
      "    def rst_features(self, fileName):\n",
      "        \n",
      "        def nucleus_BOW(text):\n",
      "            nBow=defaultdict(int)\n",
      "            for word in word_tokenize(text.decode(\"utf8\")):\n",
      "                nBow[str(('Nucleus_BOW',word))]+=1\n",
      "            return dict(nBow)\n",
      "        \n",
      "        def num_Relations(NodeList):\n",
      "            relationCount=defaultdict(int)\n",
      "            for node in NodeList:\n",
      "                if node.relation is not None:\n",
      "                    relationCount[str(('Num_Relation',node.relation))]+=1\n",
      "            return dict(relationCount)\n",
      "        \n",
      "        def relation_BOW(NodeList):\n",
      "            relationBow=defaultdict(int)\n",
      "            for node in NodeList:\n",
      "                for word in word_tokenize(node.text.decode(\"utf8\")):\n",
      "                    if node.relation is not None:\n",
      "                        relationBow[str(('Relation_BOW',node.relation,word))]+=1\n",
      "            return dict(relationBow)\n",
      "        \n",
      "        def weight_relation(NodeList):\n",
      "            relationWeight= defaultdict(int)\n",
      "            for node in NodeList:\n",
      "                if node.relation is not None:\n",
      "                    relationWeight[str(('Relation_Weight',node.relation))]+=1\n",
      "            return dict(relationWeight)\n",
      "        \n",
      "        def relation_POS(NodeList):\n",
      "            relationPos= defaultdict(int)\n",
      "            for node in NodeList:\n",
      "                for sent in sent_tokenize(node.text.decode(\"utf8\")):\n",
      "                    for (word,tag) in nltk.pos_tag(sent):\n",
      "                        relationPos[str(('Relation_POS',node.relation,tag))]+=1\n",
      "            return dict(relationPos)\n",
      "        \n",
      "        def polarityNucleus(NodeList):\n",
      "            value=defaultdict(int)\n",
      "            for node in NodeList:\n",
      "                if node.prop =='Nucleus':\n",
      "                    for word in word_tokenize(node.text.decode(\"utf8\")):\n",
      "                        if word in poswords:\n",
      "                            value['POS']+=1\n",
      "                        if word in negwords:\n",
      "                            value['NEG']+=1\n",
      "                    break\n",
      "                    \n",
      "            return dict(value)\n",
      "                                                           \n",
      "        features={}\n",
      "#         neucleus_bow={}\n",
      "#         NodeList = subParseTree.getParseTreeList(fileName)\n",
      "#         NodeList.reverse()\n",
      "#         for node in NodeList:\n",
      "#             if node.prop == 'Nucleus':\n",
      "#                 neucleus_bow=nucleus_BOW(node.text)\n",
      "#                 break\n",
      "\n",
      "#         features= dict(relation_POS(NodeList).items())\n",
      "        return features\n",
      "        \n",
      "    \n",
      "    def extract(self, document, filename=None):\n",
      "        text = utils.clean_text(document)\n",
      "        features = self.bow_features(text) #dict( self.rst_features(fileName).items() )\n",
      "        return features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    }
   ],
   "metadata": {}
  }
 ]
}