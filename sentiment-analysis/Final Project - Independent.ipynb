{
 "metadata": {
  "name": "",
  "signature": "sha256:d53eef62638736f5c2d319c2fe74554e858d40bf7497f7643d93659cea338156"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Sentiment Analysis with Structural Features\n",
      "###Rohit Pathak, Bharadwaj Tanikella\n",
      "We train a classifier with features extracted from a RST parse tree to perform sentiment analysis on user reviews. All features and the intuition behind them are discussed along the way.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from nltk.classify.scikitlearn import SklearnClassifier\n",
      "from nltk import NaiveBayesClassifier\n",
      "from nltk.classify import apply_features\n",
      "from nltk.tokenize import sent_tokenize, word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "from sklearn.svm import LinearSVC\n",
      "\n",
      "from collections import defaultdict, Counter\n",
      "import re\n",
      "import os\n",
      "import random\n",
      "import utils\n",
      "\n",
      "import sys\n",
      "sys.path.insert(0,'../RSTParser')\n",
      "import docparser\n",
      "\n",
      "reload(utils)\n",
      "reload(docparser)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Load model from file: parsing-model.pickle.gz\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 49,
       "text": [
        "<module 'docparser' from '../RSTParser/docparser.py'>"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Feature Extractor object computes features for a document."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our dataset contains 50k imdb reviews. Currently we are training a Naive Bayes Classifier with 50 random reviews and testing the accuracy on 50 other randomly selected reviews. The classification task is binary - each review is either POSitive or NEGagive.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DATA_DIR = 'data_sample/'\n",
      "\n",
      "train_files = [DATA_DIR + 'train/pos/' + f for f in os.listdir(DATA_DIR+'train/pos/') if 'edus' in f]\n",
      "train_files += [DATA_DIR + 'train/neg/' + f for f in os.listdir(DATA_DIR+'train/neg/') if 'edus' in f]\n",
      "test_files = [DATA_DIR + 'test/pos/' + f for f in os.listdir(DATA_DIR+'test/pos/') if 'edus' in f]\n",
      "test_files += [DATA_DIR + 'test/neg/' + f for f in os.listdir(DATA_DIR+'test/neg/') if 'edus' in f]\n",
      "\n",
      "random.shuffle(train_files)\n",
      "random.shuffle(test_files)\n",
      "\n",
      "def generate_labelled_data( files, feat_func ):\n",
      "    data = []\n",
      "    for i,filename in enumerate(files):\n",
      "        label = 'pos' if 'pos' in filename else 'neg'\n",
      "        f = open(filename, 'r')\n",
      "        review = f.read()\n",
      "        f.close()\n",
      "        data.append( (feat_func(review), label) )\n",
      "        if i >0 and i%100==0: print \"At %s instances\" %i,\n",
      "    return data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Bag of Words Features\n",
      "We train a SVM with simple BOW features with stopwords removed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bow_feats(document):\n",
      "    text = document.lower()\n",
      "    words = ['[[BOW]] ' + w for w in re.findall(r'\\w+', text) if not w in stopwords.words('english')]\n",
      "    return dict( Counter(words) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set = generate_labelled_data( train_files, bow_feats )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 900 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1000 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1900 instances\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now train a SVM with the training data generated above."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classif = SklearnClassifier(LinearSVC())\n",
      "classif.train(train_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "<SklearnClassifier(LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
        "     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n",
        "     random_state=None, tol=0.0001, verbose=0))>"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And test it on the test data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_set = generate_labelled_data( test_files, bow_feats )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 100 instances\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classif, test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.76\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Moving on to RST features (individual)\n",
      "We now individually try a range of features extracted from the parse tree to see how they compare against the pure BOW model."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Nucleus Bag of Words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def nucleus_bow_feats(document, tree=None, nodelist=None):\n",
      "    if tree is None and nodelist is None:\n",
      "        tree, nodelist = docparser.getParseTree(document)\n",
      "    nodelist.reverse()\n",
      "    for node in reversed(nodelist):\n",
      "        if node.prop == 'Nucleus':\n",
      "            text = node.text.lower()\n",
      "            words = ['[[NUC_BOW]] ' + w for w in re.findall(r'\\w+', text) if not w in stopwords.words('english')]\n",
      "            return dict( Counter(words).items() ) #+ bow_feats(document).items() )\n",
      "    else:\n",
      "        print \"Empty Doc?\",\n",
      "        return bow_feats(document)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set = generate_labelled_data( train_files, nucleus_bow_feats )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 900 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1000 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? At 1300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1900 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc?\n"
       ]
      }
     ],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_set = generate_labelled_data( test_files, nucleus_bow_feats )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Doc?\n"
       ]
      }
     ],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classif = SklearnClassifier(LinearSVC())\n",
      "classif.train(train_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 79,
       "text": [
        "<SklearnClassifier(LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
        "     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n",
        "     random_state=None, tol=0.0001, verbose=0))>"
       ]
      }
     ],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classif, test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.7\n"
       ]
      }
     ],
     "prompt_number": 80
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Number of Relations\n",
      "Our proposition is that more of certain kinds of relations are usually positive reviews."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def num_relations_feats(document, tree=None, nodelist=None):\n",
      "    \n",
      "    if tree is None and nodelist is None:\n",
      "        tree, nodelist = docparser.getParseTree(document)\n",
      "    relation_count=defaultdict(int)\n",
      "    relation_count['[[NUM_RELATION]] OFFSET'] = 1\n",
      "    for node in nodelist:\n",
      "        if node.relation is not None:\n",
      "            relation_count[ '[[Num_Relation]] ' + node.relation]+=1\n",
      "    return dict(relation_count)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set = generate_labelled_data( train_files, num_relations_feats )\n",
      "\n",
      "classif = SklearnClassifier(LinearSVC())\n",
      "classif.train(train_set)\n",
      "\n",
      "test_set = generate_labelled_data( test_files, num_relations_feats )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 900 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1000 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1900 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 100 instances\n"
       ]
      }
     ],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classif, test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.5\n"
       ]
      }
     ],
     "prompt_number": 91
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Relation BOW\n",
      "Intuition: words in combination of the relation that they can be more reflective of the sentiment that they are associated with. If a word is consistently in a relation, it may affect the 'degree' of that sentiment."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def relation_bow_feats(document, tree=None, nodelist=None):\n",
      "    if tree is None and nodelist is None:\n",
      "        tree, nodelist = docparser.getParseTree(document)\n",
      "    relation_bow = Counter()\n",
      "    relation_bow['[[RELATION_BOW]] OFFSET'] = 1 \n",
      "    for node in nodelist:\n",
      "        if node.relation is not None:\n",
      "            text = node.text.lower()\n",
      "            words = ['[[RELATION_BOW]] ' + node.relation + ' ' + w for w in re.findall(r'\\w+', text) if not w in stopwords.words('english')]\n",
      "            relation_bow.update(words)\n",
      "    #print relation_bow\n",
      "    return dict(relation_bow)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 99
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set = generate_labelled_data( train_files, relation_bow_feats )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 900 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1000 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1100 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1200 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1300 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1400 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1500 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1600 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1700 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1800 instances "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 1900 instances\n"
       ]
      }
     ],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classif = SklearnClassifier(LinearSVC())\n",
      "classif.train(train_set)\n",
      "test_set = generate_labelled_data( test_files, relation_bow_feats )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At 100 instances\n"
       ]
      }
     ],
     "prompt_number": 103
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.classify.accuracy(classif, test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.725\n"
       ]
      }
     ],
     "prompt_number": 104
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Accuracy\n",
      "[We will test the accuracy of our classifier incrementally. But for now, it's pretty direct.]"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Generating test set\n",
      "test_set = []\n",
      "for inst,l in generate_labelled_data( sorted((test_files['pos'] + test_files['neg']), key=lambda k: random.random()) ):\n",
      "    test_set.append( (inst,l) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 76
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Precision, Recall and F-measure"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tp= 0.0\n",
      "tn= 0.0\n",
      "fp= 0.0\n",
      "fn= 0.0\n",
      "\n",
      "results = classifier_perceptron.classify_many([fs for (fs,l) in test_set])\n",
      "actual = [l for (f,l) in test_set]\n",
      "\n",
      "i=0\n",
      "\n",
      "for i in range(len(results)):\n",
      "    if results[i]=='pos' and actual[i]=='pos':\n",
      "        tp+=1.0\n",
      "    if results[i]=='pos' and actual[i]=='neg':\n",
      "        fp+=1.0\n",
      "    if results[i]=='neg' and actual[i]=='pos':\n",
      "        fn+=1.0\n",
      "    if results[i]=='neg' and actual[i]=='neg':\n",
      "        tn+=1.0\n",
      "        \n",
      "recall = tp / (tp + fn)\n",
      "precision = tp / (tp + fp + 1e-6)\n",
      "fmeasure = 2 * recall * precision / (recall + precision + 1e-6)\n",
      "quality = {'f-measure': fmeasure, 'recall': recall, 'precision' : precision}\n",
      "\n",
      "print quality"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'recall': 0.0, 'f-measure': 0.0, 'precision': 0.0}\n"
       ]
      }
     ],
     "prompt_number": 392
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##LEFTOVERS"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "poswords = set()\n",
      "negwords = set()\n",
      "with open('sentiment-vocab.tff','r') as fin:\n",
      "    for i,line in enumerate(fin):\n",
      "        # more list and dict comprehensions!\n",
      "        kvs = {key:val for key,val in [kvp.split('=') for kvp in line.split() if '=' in kvp]}\n",
      "        if kvs['type'] == 'strongsubj':\n",
      "            if kvs['priorpolarity'] == 'negative':\n",
      "                negwords.add(kvs['word1'])\n",
      "            if kvs['priorpolarity'] == 'positive':\n",
      "                poswords.add(kvs['word1'])\n",
      "\n",
      "all_labels = ['pos', 'neg']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class FeatureExtractor():\n",
      "    \n",
      "    def __init__(self,):\n",
      "        pass\n",
      "    \n",
      "    def bow_features(self, document):\n",
      "        text = document.lower()\n",
      "        words = ['[[BOW]] '+ w for w in re.findall(r'\\w+', text) if not w in stopwords.words('english')]\n",
      "        return dict( Counter(words) )\n",
      "    \n",
      "    # TODO: Add RST features here\n",
      "    def rst_features(self, fileName):\n",
      "        \n",
      "        def nucleus_BOW(text):\n",
      "            nBow=defaultdict(int)\n",
      "            for word in word_tokenize(text.decode(\"utf8\")):\n",
      "                nBow[str(('Nucleus_BOW',word))]+=1\n",
      "            return dict(nBow)\n",
      "        \n",
      "        def num_Relations(NodeList):\n",
      "            relationCount=defaultdict(int)\n",
      "            for node in NodeList:\n",
      "                if node.relation is not None:\n",
      "                    relationCount[str(('Num_Relation',node.relation))]+=1\n",
      "            return dict(relationCount)\n",
      "        \n",
      "        def relation_BOW(NodeList):\n",
      "            relationBow=defaultdict(int)\n",
      "            for node in NodeList:\n",
      "                for word in word_tokenize(node.text.decode(\"utf8\")):\n",
      "                    if node.relation is not None:\n",
      "                        relationBow[str(('Relation_BOW',node.relation,word))]+=1\n",
      "            return dict(relationBow)\n",
      "        \n",
      "        def weight_relation(NodeList):\n",
      "            relationWeight= defaultdict(int)\n",
      "            for node in NodeList:\n",
      "                if node.relation is not None:\n",
      "                    relationWeight[str(('Relation_Weight',node.relation))]+=1\n",
      "            return dict(relationWeight)\n",
      "        \n",
      "        def relation_POS(NodeList):\n",
      "            relationPos= defaultdict(int)\n",
      "            for node in NodeList:\n",
      "                for sent in sent_tokenize(node.text.decode(\"utf8\")):\n",
      "                    for (word,tag) in nltk.pos_tag(sent):\n",
      "                        relationPos[str(('Relation_POS',node.relation,tag))]+=1\n",
      "            return dict(relationPos)\n",
      "        \n",
      "        def polarityNucleus(NodeList):\n",
      "            value=defaultdict(int)\n",
      "            for node in NodeList:\n",
      "                if node.prop =='Nucleus':\n",
      "                    for word in word_tokenize(node.text.decode(\"utf8\")):\n",
      "                        if word in poswords:\n",
      "                            value['POS']+=1\n",
      "                        if word in negwords:\n",
      "                            value['NEG']+=1\n",
      "                    break\n",
      "                    \n",
      "            return dict(value)\n",
      "                                                           \n",
      "        features={}\n",
      "#         neucleus_bow={}\n",
      "#         NodeList = subParseTree.getParseTreeList(fileName)\n",
      "#         NodeList.reverse()\n",
      "#         for node in NodeList:\n",
      "#             if node.prop == 'Nucleus':\n",
      "#                 neucleus_bow=nucleus_BOW(node.text)\n",
      "#                 break\n",
      "\n",
      "#         features= dict(relation_POS(NodeList).items())\n",
      "        return features\n",
      "        \n",
      "    \n",
      "    def extract(self, document, filename=None):\n",
      "        text = utils.clean_text(document)\n",
      "        features = self.bow_features(text) #dict( self.rst_features(fileName).items() )\n",
      "        return features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    }
   ],
   "metadata": {}
  }
 ]
}